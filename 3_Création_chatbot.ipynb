{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import re\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import csv\n",
    "import pickle\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # ou CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des vectoriseurs, de la fonction de normalisation et du dataFrame normalisé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_question = pd.read_pickle('dtm_question.pk')\n",
    "dtm_reponse = pd.read_pickle('dtm_reponse.pk')\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french')\n",
    "stopwords = list(nltk_stopwords)\n",
    "stemmer=nltk.stem.SnowballStemmer('french')\n",
    "\n",
    "#data \n",
    "data = pd.read_csv('dataQR.csv', sep=';')\n",
    "\n",
    "#data normalisée \n",
    "data_norm = pd.read_csv('dataQR_normalize.csv', sep=';')\n",
    "\n",
    "def nettoyage_texte(txt):\n",
    "    #on met tout le texte en minuscule, et on remplce les retours à la ligne par des espaces\n",
    "    txt = txt.lower().replace('\\n', ' ')\n",
    "\n",
    "    #on retire tous les accents \n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore')\n",
    "\n",
    "    #on garde uniquement les caractères alphanumériques : \n",
    "    txt = re.sub('[^a-z_]', ' ', str(txt))\n",
    "    \n",
    "    #on retire les stops words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in nltk_stopwords)]\n",
    "    \n",
    "    #pour avoir les mots racines\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(texte) : \n",
    "    #liste des valeurs des similarités \n",
    "    similarity = []\n",
    "    \n",
    "    #liste des valeurs des index \n",
    "    index_similarity = []\n",
    "    \n",
    "    \n",
    "    #on teste la similarité avec les questions de notre base Vinted \n",
    "    dtm_q = dtm_question.transform(data_norm['Question_normalize'])\n",
    "    req_vect_q = dtm_question.transform(pd.Series([nettoyage_texte(texte)]))\n",
    "    \n",
    "    query_corpus_sim_quest = np.squeeze(cosine_similarity(dtm_q, req_vect_q))\n",
    "    \n",
    "    #on prend la plus grande similarité (attention si plusieurs)\n",
    "    sim_quest = np.max(query_corpus_sim_quest)\n",
    "    #on l'ajoute à la liste \n",
    "    similarity.append(sim_quest)\n",
    "    \n",
    "    #index de la valeur la plus grande\n",
    "    sim_quest_ind = np.argmax(query_corpus_sim_quest)\n",
    "    #on l'ajoute à la liste \n",
    "    index_similarity.append(sim_quest_ind)\n",
    "    \n",
    "    #on teste la similarité avec les réponses de notre base Vinted \n",
    "    dtm_r = dtm_reponse.transform(data_norm['Reponse_normalize'])\n",
    "    req_vect_r = dtm_reponse.transform(pd.Series([nettoyage_texte(texte)]))\n",
    "    \n",
    "    query_corpus_sim_rep = np.squeeze(cosine_similarity(dtm_r, req_vect_r))\n",
    "    \n",
    "    sim_rep = np.max(query_corpus_sim_rep)\n",
    "    similarity.append(sim_rep)\n",
    "    \n",
    "    sim_rep_ind = np.argmax(query_corpus_sim_rep)\n",
    "    index_similarity.append(sim_rep_ind)\n",
    "    \n",
    "    if max(similarity) > 0.50 : \n",
    "        return data.iloc[index_similarity[similarity.index(max(similarity))],1] + '\\n'\n",
    "    else : \n",
    "        return 'autre_question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chatbot() : \n",
    "    #on récupère la question posée par l'utilisateur \n",
    "    phrase_user = input(\"-Chatbot : \\nSalut, comment puis-je t'aider ? \\n-User : \\n\\n\")\n",
    "    \n",
    "    #boucle while permettant la continuité du dialogue\n",
    "    #si l'utilisateur nous dit au revoir alors le programme s'arrête \n",
    "    while not re.search(r\"(.*)au revoir\", phrase_user, re.IGNORECASE):\n",
    "        #on normalise la phrase entrée par l'utilisateur \n",
    "        phrase_norm = nettoyage_texte(phrase_user)\n",
    "        \n",
    "        if similarity(phrase_norm) == 'autre_question' : \n",
    "            phrase_user = input(\"-Chatbot : \\nPose une autre question \\n-User : \\n\\n\")\n",
    "        else : \n",
    "            phrase_user = input(similarity(phrase_norm) + '\\n' + \"As-tu d'autres questions ? \\n-User : \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
